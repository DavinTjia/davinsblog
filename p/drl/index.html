<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A note on deep robotics learning (UW CSE 599 G1)"><title>Deep Robotics Learning (1) - Introduction and Policy Gradient</title><link rel=canonical href=https://davintjia.github.io/p/drl/><link rel=stylesheet href=/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="Deep Robotics Learning (1) - Introduction and Policy Gradient"><meta property="og:description" content="A note on deep robotics learning (UW CSE 599 G1)"><meta property="og:url" content="https://davintjia.github.io/p/drl/"><meta property="og:site_name" content="Davin Tjia"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="CSE599 G1 Note"><meta property="article:published_time" content="2023-03-07T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-07T00:00:00+00:00"><meta property="og:image" content="https://davintjia.github.io/p/drl/cover.jpg"><meta name=twitter:title content="Deep Robotics Learning (1) - Introduction and Policy Gradient"><meta name=twitter:description content="A note on deep robotics learning (UW CSE 599 G1)"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://davintjia.github.io/p/drl/cover.jpg"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/pfp_hu3942346a14d3c629e18ce811e97b9a78_1782930_300x0_resize_box_3.png width=300 height=299 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>ðŸ¤–</span></figure><div class=site-meta><h1 class=site-name><a href=/>Davin Tjia</a></h1><h2 class=site-description>Doing research in WEIRD lab @ UW CS</h2></div></header><ol class=social-menu><li><a href=mailto:davin05@cs.washington.edu target=_blank title=Email rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 5m0 2a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><path d="M3 7l9 6 9-6"/></svg></a></li><li><a href=https://twitter.com/TjiaDavin target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Blog Posts</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#policy-gradient>Policy Gradient</a><ol><li><a href=#variance-reduction-for-pg>Variance Reduction for PG</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/drl/><img src=/p/drl/cover_hub3ab9d1f7e608d31f3a81928ae7131ca_103491_800x0_resize_q75_box.jpg srcset="/p/drl/cover_hub3ab9d1f7e608d31f3a81928ae7131ca_103491_800x0_resize_q75_box.jpg 800w, /p/drl/cover_hub3ab9d1f7e608d31f3a81928ae7131ca_103491_1600x0_resize_q75_box.jpg 1600w" width=800 height=450 loading=lazy alt="Featured image of post Deep Robotics Learning (1) - Introduction and Policy Gradient"></a></div><div class=article-details><header class=article-category><a href=/categories/lecture-notes-expansion/>Lecture Notes Expansion</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/drl/>Deep Robotics Learning (1) - Introduction and Policy Gradient</a></h2><h3 class=article-subtitle>A note on deep robotics learning (UW CSE 599 G1)</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Mar 07, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>11 minute read</time></div></footer></div></header><section class=article-content><p>In this blog post, I would be going over some topics on deep reinforcement learning with focus on robotics learning. I would
expect this to be spanned across a series of posts. The content of these posts is my takeaway and additional exploration
on the topics taught in <a class=link href=https://courses.cs.washington.edu/courses/cse599g/23wi/ target=_blank rel=noopener>CSE599 G1</a> at University of Washington
by <a class=link href=https://homes.cs.washington.edu/~abhgupta/ target=_blank rel=noopener>Abishek Gupta</a>. Abishek is such a great teacher and this is such a great
class that I think it deserves documentation of some form. That is the motivations of this blog posts series.</p><p><strong>Disclaimer</strong>: if there is any error in this blog posts, it is entirely my mistakes and my misunderstanding, but not
the results of the slides of the lectures or Abhishek Gupta&rsquo;s delivery. Please contact me if you have any concerns or
questions about the content of this post. Thank you!</p><h2 id=introduction>Introduction</h2><p>To start, I would like to discuss some motivation on why reinforcement learning and why deep learning.</p><p>Although the concepts of learning is not new, the field robotics have much longer history in the classical methods. However, without learning, robotics is a much more complicated and specific tasks and requires multiple
layers of engineering across different discipline, For example, robots that have been built and coded to navigate one
environment might fail entirely at another (although similar) environment. This is because hard coded functionality,
perception, and planning is much less generalizable in exchange for precision and reliability. However, such domain-specific
design has many draw backs. It is much less economically justifiable (than, says, deploy a real human) and less likely
to have a wide-spread deployment. This is where the motivation of learning comes in. Instead of building a finite state
machine that transitions based on tasks, it is much better to train a model that would adapt based on the diversity in
the data. Realistically speaking, the most likely solutions would fall in somewhere in between: we would like to
modularize robotic engineering where each module perhaps consists of some learned models that are good at a tasks but
are more generalization than state machine.</p><p>Hopefully, this would convince you a bit on maybe we should explore the possibilities of using learning in robotics.
Assuming that you are buying the concepts of learning, you might want to ask why reinforcement learning (RL)? Supervised
learning is much more extensively studied and seems to work phenomenonly well based on recent progress on CV and NLP.
For supervised learning to perform well, some typical requires include 1. the data much be collected iid from the
distribution 2. a lot of data, like a lot of data, as much as possible 3. well labeled data. These requirements
do not fit well with the nature of robotic tasks. For 1., robotics tasks is usually sequential, that means it is not iid.
For 2, it is exponentially more difficult to collect data. Different labs use different robots, under different
environments, and with different calibration. There is lack of a general real-world data set that most researchers
generally agree on like ImageNet in CV in early 2010s. For 3., it is hard to label the data. For example, a state
might be good in one sequence of action but might be horrible in another sequence of action. The label of a particular
data points need to take in multiple factors for considerations such as current goal, current state and available actions,
and current trajectory. This make supervised learning generally an unsuitable way for robot to learn novel tasks.</p><p>Reinforcement learning on the other hand is a much more natural way to learn a sequence of action for a goal. In
psychology there are two main category of conditioning: classical and operant. Classical conditioning is very well
known thanks to Pavlov&rsquo;s dog. Operant conditioning, in simple term, refers to using rewards or punishments to incentive
good behaviors. B.F. Skinner, one of the main figure in behavior psychology, believes that all behavior is learned and
novel reinforcement shapes novel behaviors. Now, if only there is a way that we can use rewards or punishments to
reinforce the robots to behave. Reinforcement learning perfectly fits that description. (I would assume
most readers have basic knowledge in RL from now on).</p><p>If I successfully (kinda) convince you that learning, specifically reinforcement learning, might be
part of the solution to robotics, the next step is to get you believe that we should also combine
deep learning with RL. One most obvious example is <a class=link href=https://www.nature.com/articles/nature16961 target=_blank rel=noopener>AlphaGo</a>.
The main ideas of AlphaGo is combining deep Q-learning with Monte Carlo Tree Search (MCTS). A
non-neural way to do Q-learning is to uses a dictionary (or some other cleverer data structure) to
store the values of all the state-action pairs. However, this quickly become intractable when the
possible combinations exceed the number of atoms in the universe like the game of Go. It would be
much better if we have a parameterized function that could takes in state-action pairs and spits out
their (estimated) values. Similarly, hand-crafting the agents and models are difficult and expensive.
Using a neural network to represent the models would be much simpler to do (sometimes it is as
easy as importing an out-of-the-box ResNet from the library!).</p><p>Now at the end of introduction, I would also like to raise some counterarguments of why we shouldn&rsquo;t
always be using deep reinforcement learning. Typical problems with deep learning such as low
interpretability, data inefficiency, and sensitivity to hyperparameters still apply. In addition,
deep reinforcement learning (or RL in general) are not suitable for tasks that requires high precisions.
The values from the output is often just an estimation (albeit a good one) and many other factors
including sampling strategy can all affect the final trajectories. In industry such as manufacturing
where a robot only needs to master one tasks well and efficiently or medicine where safety and interpretability is critical
are not (yet) good areas where DRL should be prioritized.</p><h2 id=policy-gradient>Policy Gradient</h2><p>To use deep learning on reinforcement learning, identifying objective function for optimization is the first crucial step. Recall that in RL we have a policy $\pi$ that takes a state $s$ as input and output an action $a$. The hope is that the trajectories $\tau$&rsquo;s (a sequence of states and actions) generated according $\pi$ would optimize the return $r$. This gives us
$$
\max_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}} [\sum^T_{t = 0 } r (s_t, a_t)]
$$
There are many techniques that can be used to optimize over this objective functions. Different techniques requires different modifications to the objectives and results in different algorithms. Here, we would be focusing on using gradient <em>asscent</em>, which in the setting of RL is often known as <strong>policy gradient</strong>. Equivalently, the continuous version of the sum above is given by
$$
J(\theta) = \int p_{\theta } (\tau ) R(\tau )d\tau
$$
This induce the REINFORCE algorithm where we estimate the policy gradient with likelihood ratio
$$\begin{aligned}
\nabla _{\theta } J(\theta) &= \nabla_{\theta} \int p_{\theta } (\tau ) R(\tau )d\tau \\\
&= \int \nabla_{\theta} p_{\theta } (\tau ) R(\tau )d\tau \\\
&= \int \frac{p _{\theta}(\tau)}{p_\theta (\tau) } \nabla_{\theta} p_{\theta } (\tau ) R(\tau )d\tau \\\
&= \int \frac{p _{\theta}(\tau)}{ p_\theta (\tau) } \nabla_{\theta} p_{\theta } (\tau ) R(\tau )d\tau \\\
&= \int p_\theta (\tau) \nabla_{\theta} \log p_{\theta } (\tau ) R(\tau )d\tau \quad \text{REINFORCE trick} \\\
&= \mathbb{E } _{p_{\theta }(\tau )} [ \nabla_{\theta} \log p_{\theta }(\tau )R(\tau)]
\end{aligned}$$
Now we unroll the definition of $p_{\theta }(\tau )$
$$
p_{\theta }(\tau ) = p(s_0) \prod^{T- 1 }_{t= 0 }p(s_{t+ 1} | s_t, a_t) \pi(a_t |s_t)
$$
where $p$ is the environment dynamics (an abuse of notation) and $\pi$ is the policy. Using property of log, we have
$$\begin{aligned}
\log p_{\theta }(\tau ) &= \log p(s_0) + \sum^{T- 1 }_{t= 0 } (\log p(s_{t+ 1} | s_t, a_t) + \log \pi(a_t |s_t)) \\
\nabla_{\theta} \log p_{\theta }(\tau ) &= \nabla_{\theta} \log p(s_0) + \sum^{T- 1 }_{t= 0 } ( \nabla_{\theta} \log p(s_{t+ 1} | s_t, a_t) + \nabla_{\theta} \log \pi(a_t |s_t))
\end{aligned}$$
Now, we assume <em>model free</em> learning, that is, only the policy is parameterized. This means the dynamic $p$ is independent of $\theta$, which leaves us<br>$$\begin{aligned}
\nabla_{\theta} \log p_{\theta }(\tau ) &= \nabla_{\theta} \log p(s_0) + \sum^{T- 1 }_{t= 0 } ( \nabla_{\theta} \log p(s_{t+ 1} | s_t, a_t) + \nabla_{\theta} \log \pi(a_t |s_t)) \\\
&= 0 + \sum^{T- 1 }_{t= 0 } (0 + \nabla_{\theta} \log \pi(a_t |s_t)) \\\
&= \sum^{T-1 }_{t =0} \nabla_{\theta} \log \pi(a_t |s_t)
\end{aligned}$$
We can then rewrite our objectives as
$$
\mathbb{E } _{p_{\theta }(\tau )} [ \nabla_{\theta} \log p_{\theta }(\tau )R(\tau)] = \mathbb{E } _{p_{\theta }(\tau )} [ \sum^{T-1 }_{t =0} \nabla_{\theta} \log \pi(a_t |s_t) \sum^{T}_{t=0}r(s_t, a_t)]
$$
where the reward $R$ collected along the trajectory $\tau$ is given by $r(s_t, a_t)$ at each time stamp from $t = 0$ to the horizon $t=T$. Although the later expectation seems intractable to compute, we can approximate it with sampling !
$$
\mathbb{E } _{p_{\theta }(\tau )} [ \sum^{T-1 }_{t =0} \nabla_{\theta} \log \pi(a_t |s_t) \sum^{T}_{t=0}r(s_t, a_t)] \approx \frac{1 }{N }\sum^N_{i = 0 }\sum^T_{t = 0 } \nabla_{\theta} \log \pi(a_t^i |s_t^i) \sum^{T}_{t^{\prime}=0}r(s_{t^{\prime}}^i, a_{t^{\prime}}^i)
$$
This is essentially gives us the REINFORCE algorithm: you sample trajectories $\tau^i$ from current policy $\pi_{\theta} (a_t | s_t)$ and then estimate the gradient $ \nabla_{\theta}J(\theta)$ and then we update the current parameter $\theta \leftarrow \theta + \nabla_{\theta}J(\theta)$ with gradient ascent:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def REINFORCE:
</span></span><span class=line><span class=cl>  sample tau from pi_theta 
</span></span><span class=line><span class=cl>  gradient &lt;- estimated by sampling
</span></span><span class=line><span class=cl>  theta &lt;- current theta + lr * gradient
</span></span></code></pre></td></tr></table></div></div><p>In plain word, we want to select good data and then increase the likelihood of selecting good data. While policy gradient is unbiased, it is nontheless a <em>high variance</em> estimator. Because at one time we use a single sample (one trajectory) estimate but we really want an averaged return (averages across many trajectories) estimate.</p><h3 id=variance-reduction-for-pg>Variance Reduction for PG</h3><p>The most simple way to address the problem of high variance is taken &ldquo;causaility&rdquo; in mind. Notice in the sampling equation,
$$
\frac{1 }{N }\sum^N_{i = 0 }\sum^T_{t = 0 } \nabla_{\theta} \log \pi(a_t^i |s_t^i) \sum^{T}_{t^{\prime}=0}r(s_{t^{\prime}}^i, a_{t^{\prime}}^i)
$$
the return is summing accross all $t^{\prime} \in [0, T]$ at each time stamp $t$. This means the trajectory depends on the past and the future, but at a given moment $t^{\prime}$ what has been done has been done and we only care about the return we get in the future. Therefore, we can consider <strong>return to go</strong> by ignoring past term and update our sampling equation to
$$
\frac{1 }{N }\sum^N_{i = 0 }\sum^T_{t = 0 } \nabla_{\theta} \log \pi(a_t^i |s_t^i) \sum^{T}_{t^{\prime}=t}r(s_{t^{\prime}}^i, a_{t^{\prime}}^i)
$$
where we excluding past term and now $t^{\prime} \in [t, T]$. This method, however, doesn&rsquo;t solve the problem of &ldquo;arbitrary centering&rdquo;. When some part of the trajectories returns negative rewards and other part return positive rewards, it is clear where to center the distribution around actions where the reward is positive. In the scenario where all the rewards are positive (which is more common), every actions in the distribution would be pushed up. To select good actions, we need to be very precise about pushing up the good ones more than the other, which is difficult and has high variance. Now, what if instead of pushing up all actions, we push down the actions that are bad and pushed up the actions that are good even though they all receive positive rewards? Can we reduce the variance further. The idea is to introduce a current state dependent function, what we called <strong>baseline</strong> $b(s_t)$, and subtracting it from the return sum in policy gradient.
$$
\frac{1 }{N }\sum^N_{i = 0 }\sum^T_{t = 0 } \nabla_{\theta} \log \pi(a_t^i |s_t^i) \sum^{T}_{t^{\prime}=t}[r(s_{t^{\prime}}^i, a_{t^{\prime}}^i) - b(s_t)]
$$
Baseline allows us to center the return (at current state) to reduce the variance. But do we sacrifice lower variance with higher bias by introducing the baseline term? Actually, no.
$$
\begin{aligned}
\int p\left(\tau \right) \nabla_\theta \log \pi_\theta\left(\tau \right)\left[\sum_{t^{\prime}=t}^T r\left(\tau\right)-b\left(s_t\right)\right] d\tau = &\int_{\mathcal{S}} \int_{\mathcal{A}} p\left(s_t, a_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left[\sum_{t^{\prime}=t}^T r\left(s_{t^{\prime}}, a_{t^{\prime}}\right)-b\left(s_t\right)\right] d s_t d a_t \\
= &\int_{\mathcal{S}} \int_{\mathcal{A}} p\left(s_t, a_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left[\sum_{t^{\prime}=t}^T r\left(s_{t^{\prime}}, a_{t^{\prime}}\right)\right] d s_t d a_t - \\
&\int_{\mathcal{S}} \int_{\mathcal{A}} p\left(s_t, a_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) b\left(s_t\right) d s_t d a_t
\end{aligned}
$$
This means that if we can show the integral with baseline as the integrand is 0, it would mean this is still an unbiased estimator.
$$
\begin{aligned}
\iint p\left(s_t, a_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left[b\left(s_t\right)\right] d s_t d a_t &=\iint p\left(s_t\right) \pi_\theta\left(a_t \mid s_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left[b\left(s_t\right)\right] d s_t d a_t \\
& =\int p\left(s_t\right) b\left(s_t\right) \int \pi_\theta\left(a_t \mid s_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) d a_t d s_t \\
& =\int p\left(s_t\right) b\left(s_t\right) \int \nabla_\theta \pi_\theta\left(a_t \mid s_t\right) d a_t d s_t \\
&=\int p\left(s_t\right) b\left(s_t\right) \nabla_\theta \int \pi_\theta\left(a_t \mid s_t\right) d a_t d s_t\\
&=\int p\left(s_t\right) b\left(s_t\right) \nabla_\theta(1) d s_t \quad \text{$\pi$ is a valid probability distribution}\\
&=0 \quad \text{derivative w.r.t any number is 0}
\end{aligned}
$$
Indeed, this is a rare day in machine learning where adding the baseline term reduces the variance without trading off more bias. But how do we find this function $b$? Well, remember we spend quite sometime on convincing you that we should be using deep learning in RL? DL provides an intuitive solution: we <em>learn</em> it with deep neural net! (However, you can still learn baseline with sampling) On the side note, for those who knows Advantage function in Q-learning,
$$
A(s, a) = Q(s, a) - V(s)
$$
which is nothing more than just subtracting current $Q$ value by current value function. Notice the resemblance between this and baseline, you can interpret advantage function as an estimator with baseline embedded by default.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
<a href=/tags/deep-learning/>Deep Learning</a>
<a href=/tags/cse599-g1-note/>CSE599 G1 Note</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2023 Davin Tjia</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>